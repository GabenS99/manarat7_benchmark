# MANARAT7 BENCHMARK

**For ALLaM and JAIS Model Providers**

A comprehensive benchmarking pipeline for evaluating your Arabic language models on Islamic domain knowledge. This repository is designed for **model providers** to test and benchmark their local ALLaM and JAIS models.

## üéØ Overview

This evaluation framework tests Arabic language models on Islamic knowledge using three question types:
- **MCQ** (Multiple Choice Questions)
- **COMP** (Comprehension/Text Analysis)
- **KNOW** (Knowledge Questions)

Each question type includes three difficulty levels: Beginner, Intermediate, and Advanced.

## üöÄ Quick Start for Providers

### Prerequisites
- Python 3.8+ 
- **GPU with 14GB+ VRAM** (required for local models)
- 32GB+ RAM (recommended for model loading)
- Internet connection (for initial model download from Hugging Face)

### ‚ö†Ô∏è CRITICAL: Transformers Version Incompatibility

**ALLaM and Jais-2 models require DIFFERENT transformers versions and CANNOT be used together:**

- **ALLaM Models**: Require `transformers>=4.45.0` (stable)
- **Jais-2 Models**: Require `transformers 5.0` (bleeding-edge)

**Solution**: Use separate virtual environments or choose only one model type per deployment.

### 1. Clone and Setup Environment

```bash
# Clone the repository
git clone <repository-url>
cd manarat7_benchmark

# Automated setup (recommended)
python3 setup_venv.py

# Manual setup (alternative)
python3 -m venv islamgpt_env
source islamgpt_env/bin/activate  # Linux/macOS
# islamgpt_env\Scripts\activate  # Windows
pip install -r requirements.txt
```

### 2. Configure Hugging Face Access

```bash
# Copy environment template
cp env_template.txt .env

# Add your Hugging Face token (required for model access)
echo "HUGGINGFACE_TOKEN=hf_your_token_here" >> .env
```

**Get your token**: https://huggingface.co/settings/tokens

> **Note**: Cloud API keys (OpenAI, Gemini, etc.) are optional and only needed if you want to compare against cloud models.

### 3. Configure Your Model

Edit `config.yaml` to specify which model to evaluate:

**For ALLaM Providers (ALLaM-34B):**
```yaml
prediction_models:
  local_allam: ["/path/to/your/allam-34b-model"]  # REPLACE with your actual model path
  # IMPORTANT: Comment out other local models:
  # local_jais: [jais_13b]
  # local_jais2: [inceptionai/Jais-2-8B-Chat]
```

**Model Path Configuration:**
- **Absolute path**: `/home/user/models/allam-34b` or `/data/models/allam-34b`
- **Relative path**: `./models/allam-34b` (if model is in project directory)
- **Path requirements**: The directory must contain:
  - `config.json` (model configuration)
  - `tokenizer.json` or `tokenizer_config.json` (tokenizer files)
  - Model weight files (`.bin`, `.safetensors`, or similar)
- **Verification**: Check that the path exists: `ls /path/to/your/allam-34b-model`

**For JAIS Providers (Jais-2-70B):**
```yaml
prediction_models:
  local_jais2: [inceptionai/Jais-2-70B-Chat]  # Jais-2-70B model
  # IMPORTANT: Comment out other local models:
  # local_allam: [ALLaM-AI/ALLaM-7B-Instruct-preview]
  # local_jais: [jais_13b]
```

**Request model access** on Hugging Face:
- Jais-2-70B: https://huggingface.co/inceptionai/Jais-2-70B-Chat
- Click "Agree and access repository" and wait 1-2 minutes for approval

### 4. Verify Your Model Setup

**For ALLaM Providers (ALLaM-34B):**
```bash
# 1. Verify model path exists
ls /path/to/your/allam-34b-model

# 2. Test model loading
python3 test_allam_local.py
```
Expected: Model loads successfully and generates responses ‚úÖ

**For JAIS Providers (Jais-2-70B):**
```bash
# 1. Verify transformers upgrade (REQUIRED)
python3 -c "import transformers; print(transformers.__version__)"
# Should show: 5.0.0.dev0 or similar (NOT 4.x.x)

# 2. If not upgraded, upgrade now:
pip install --upgrade git+https://github.com/huggingface/transformers.git

# 3. Verify setup
python3 verify_jais2_setup.py
```
Expected: 7/7 checks passed ‚úÖ

### 5. Run Evaluation

```bash
# Activate environment
source islamgpt_env/bin/activate

# Run full evaluation pipeline
python3 scripts/main.py
```

The pipeline will:
1. Load your model into GPU memory
2. Process all questions (75 files, ~3,500+ questions)
3. Save predictions in `results/predictions/`
4. Generate performance statistics


## üîß Provider Configuration


### Evaluation Parameters (Optimized for Local Models)
```yaml
prediction_parameters:
  temperature: 0          # Deterministic output
  max_tokens: 2000       # Response length limit
  few_shots: false       # Enable/disable few-shot examples
  show_cot: false        # Chain of thought reasoning
  batch_save_size: 25    # Save every N questions
  checkpoint_enabled: true # Resume if interrupted
  max_parallel_workers: 1  # CRITICAL: Always 1 for GPU models
```

### Data Selection
```yaml
data_to_predict:
  question_types: [KNOW, COMP, MCQ]  # All question types
  difficulty_levels: [Beginner, Intermediate, Advanced]  # All levels
```


## üìÅ Output Structure

Results are saved in organized directories:

```
results/
‚îú‚îÄ‚îÄ predictions/           # Model predictions
‚îÇ   ‚îú‚îÄ‚îÄ MCQ/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ A_beginner/   # JSON + Excel outputs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ B_intermediate/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ C_advanced/
‚îÇ   ‚îú‚îÄ‚îÄ COMP/
‚îÇ   ‚îî‚îÄ‚îÄ KNOW/
‚îú‚îÄ‚îÄ statistics/           # Performance metrics
‚îî‚îÄ‚îÄ evaluations/         # Evaluation results (if enabled)
```


**Ready to benchmark your model! üöÄ**
